{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a MultiLayerPerceptron from first principles\n",
    "---\n",
    "This notebook implements a simple MLP model class using only numpy. Minibatch gradient descent using backpropogation is used for training (with fixed learning rate).\n",
    "\n",
    "The gradient of the activation functions is hard coded (as opposed to using autograd aka autodiff implementation.\n",
    "\n",
    "Only fully connected dense layers are supported.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "Implement a MultiLayerPerceptron class with the following structure\n",
    "\n",
    "- Input arguments will be \n",
    "    * layers (first layer must be input aka passthrough), which is a list of ints, giving number of nuerons in each layer. Each layer (except for output) will include bias term automatically\n",
    "    * activations, a list of functions which will be used as activations at each layer. Probably these functions will be methods of the class and specified as strings\n",
    "- It will have private methods\n",
    "    * forward - for making a pass through the network and computing all intermediate output\n",
    "    * backward - computing gradients on backward pass through the network and updating weights and biases\n",
    "    * update_mini_batch - updating weights and biases based on a minibatch\n",
    "    * sigmoid, relu, softmax - activations and their derivatives (Jacobians). (Maybe put these in separate class?)\n",
    "    * metrics for eval (maybe put these in separate class?)\n",
    "    * loss functions: cross entropy, square error, absolute error (maybe put into separate class)\n",
    "- It will have public methods\n",
    "    * fit - perform mini-batch gradient descent to fit the model on a dataset\n",
    "    * predict - pass input through network and generate prediction, no need to store intermediate results \n",
    "    \n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------\n",
    "# Define MLP class ------------------\n",
    "#------------------------------------\n",
    "\n",
    "# here is the main class\n",
    "# TODO: Currently backward only works on one instance at a time\n",
    "# so don't get any matrix speedups when working on a minibatch\n",
    "# need to go through and alter all grad functions so that they can\n",
    "# handle mini-batch at a time\n",
    "\n",
    "class MultiLayerPerceptron:\n",
    "    \"\"\"\n",
    "    A simple implementation of multi layer perceptron\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers, activations, loss):\n",
    "        \"\"\"\n",
    "        Create instance of MLP model\n",
    "        \n",
    "        :param layers: A list of ints giving number of neurons in each layer. Note first entry of list should match size of expected input, and final layer is output\n",
    "        :param activations: A list of strings specifying activation function to use in each layer. Must have length ``len(layers)-1`` since first layer is a passthrough. \n",
    "            allowed activations are: \"relu\", \"logistic\", \"tanh\", \"softmax\"\n",
    "        :return: MultiLayerPerceptron\n",
    "        \"\"\"\n",
    "        \n",
    "        # weights is a list of numpy arrays. Note assumes W @ x, so W.shape[1] == number of nuerons in previous layer\n",
    "        self.weights = [np.random.randn(this_layer_size, prev_layer_size) for this_layer_size, prev_layer_size in zip(layers[1:], layers[:-1])]\n",
    "        \n",
    "        # biases is a list of numpy arrays with single column\n",
    "        self.biases = [np.random.randn(this_layer_size,1) for this_layer_size in layers[1:]]\n",
    "        \n",
    "        # activations is a list of callable functions specified by the strings provided\n",
    "        self.activations = [getattr(self, '_' + x) for x in activations]\n",
    "        \n",
    "        # loss is a string specifying the name of loss function\n",
    "        self.loss = getattr(self, '_' + loss)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Generate predictions with current weights on instance x\n",
    "        \"\"\"\n",
    "        \n",
    "        if x.ndim==1:\n",
    "            # make column vectors so matrix multiplication works correctly\n",
    "            x = x.reshape(-1,1)\n",
    "        elif x.ndim==2 & (x.shape[0] == 1 or x.shape[1] ==1):\n",
    "            # we either have a columns vector or row vector, lets ensure we have a column\n",
    "            x = x.reshape(-1,1)\n",
    "        elif x.ndim==2:\n",
    "            # most likely we have observations in rows, we should transpose so below code runs\n",
    "            x = x.T\n",
    "        else:\n",
    "            raise ValueError(\"Shape of input is not appropriate. See docstring.\")\n",
    "            \n",
    "        for w, b, a in zip(self.weights, self.biases, self.activations):\n",
    "            z = w @ x + b\n",
    "            x = a(z, False)\n",
    "        return x.T if x.shape[1] != 1 else x\n",
    "    \n",
    "    def stochastic_gradient_descent(self, x, y, batch_size=32, epochs=1, eta=0.1, n_print=100, valid_data=None, metric=None):\n",
    "        \"\"\"\n",
    "        Implement mini-batch stochastic gradient descent to train the network\n",
    "        \"\"\"\n",
    "        \n",
    "        # parse optional arguments\n",
    "        if valid_data is not None:\n",
    "            x_valid, y_valid = valid_data[0], valid_data[1]\n",
    "        if metric is not None:\n",
    "            metric_fn = getattr(self, '_' + metric)\n",
    "        \n",
    "        # initialise output store\n",
    "        history = {}\n",
    "        history['loss'] = []\n",
    "        if (metric is not None) & (valid_data is not None):\n",
    "            history['metric'] = []\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            # shuffle the data\n",
    "            idx = np.random.choice(np.arange(x.shape[0]), size=x.shape[0], replace=False)\n",
    "            x_ = x[idx, :]\n",
    "            y_ = y[idx, :]\n",
    "            num_batches = np.int(x.shape[0] / batch_size)\n",
    "            if num_batches > 100:\n",
    "                for k in tqdm(range(num_batches)):\n",
    "                    s = k * batch_size\n",
    "                    e = (k+1) * batch_size\n",
    "                    x_batch, y_batch = x_[s:e], y_[s:e]\n",
    "                    weight_deltas, bias_deltas = [], []\n",
    "                    for xi, yi in zip(x_batch, y_batch):\n",
    "                        w, b = self._two_pass(xi, yi)\n",
    "                        weight_deltas.append(w)\n",
    "                        bias_deltas.append(b)\n",
    "                    for i, (weight, bias) in enumerate(zip(self.weights, self.biases)):\n",
    "                        for w_delta, b_delta in zip(weight_deltas, bias_deltas):\n",
    "                            weight -= (eta/batch_size) * w_delta[i]\n",
    "                            bias -= (eta/batch_size) * b_delta[i]    \n",
    "            else:\n",
    "                for k in range(num_batches):\n",
    "                    s = k * batch_size\n",
    "                    e = (k+1) * batch_size\n",
    "                    x_batch, y_batch = x_[s:e], y_[s:e]\n",
    "                    weight_deltas, bias_deltas = [], []\n",
    "                    for xi, yi in zip(x_batch, y_batch):\n",
    "                        w, b = self._two_pass(xi, yi)\n",
    "                        weight_deltas.append(w)\n",
    "                        bias_deltas.append(b)\n",
    "                    for i, (weight, bias) in enumerate(zip(self.weights, self.biases)):\n",
    "                        for w_delta, b_delta in zip(weight_deltas, bias_deltas):\n",
    "                            weight -= (eta/batch_size) * w_delta[i]\n",
    "                            bias -= (eta/batch_size) * b_delta[i]    \n",
    "            \n",
    "            y_hat = self.predict(x)\n",
    "            curr_loss = self.loss(y, y_hat)\n",
    "            history['loss'].append(curr_loss)\n",
    "            if metric is not None:\n",
    "                curr_metric = metric_fn(y, y_hat)\n",
    "                history['metric'].append(curr_metric)\n",
    "            if (epoch+1) % n_print == 0: print(f\"SGD | Epochs completed: {epoch+1} | Training set loss: {curr_loss}\")\n",
    "        return history\n",
    "                    \n",
    "        \n",
    "    def _forward(self, x: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        forward pass through the network, record all intermediate computations for use later in backwards pass\n",
    "        \n",
    "        :param x: np.ndarray. If x.ndim == 1, then x is a vector with layers[0] elements. If x.ndim == 2, then shape should be n_observation, layers[0]. Ie. One row per obs.\n",
    "        :param y: np.ndarray. x.ndim==1, then y.ndim==1 and has layers[-1] elements. If x.ndim == 2, then y.ndim == 2 and shape of y should be n_observation, layers[-1]. Ie. One row per obs.\n",
    "        :return: Tuple of lists (layer_outputs, layer_grads)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert y.ndim == x.ndim\n",
    "        if x.ndim==1:\n",
    "            # make column vectors so matrix multiplication works correctly\n",
    "            x = x.reshape(-1,1)\n",
    "            y = y.reshape(-1,1)\n",
    "        elif x.ndim==2 & (x.shape[0] == 1 or x.shape[1] ==1):\n",
    "            # we either have a columns vector or row vector, lets ensure we have a column\n",
    "            assert y.ndim == x.ndim\n",
    "            x = x.reshape(-1,1)\n",
    "            y = y.reshape(-1,1)\n",
    "        elif x.ndim==2 & np.min(x.shape) > 1:\n",
    "            # assume we have observations in rows, we should transpose to below code runs\n",
    "            x = x.T\n",
    "            y = y.T\n",
    "        else:\n",
    "            raise ValueError(\"Shape of input is not appropriate. See docstring.\")\n",
    "            \n",
    "        \n",
    "        # store output from each layer in this object\n",
    "        # no gradient at input\n",
    "        layer_outputs = [x]\n",
    "        grads = []\n",
    "        \n",
    "        # pass through all layers\n",
    "        for w, b, a in zip(self.weights, self.biases, self.activations):\n",
    "            z = w @ x + b\n",
    "            x, g = a(z, True)\n",
    "            layer_outputs.append(x)\n",
    "            grads.append(g)\n",
    "        \n",
    "        # pass through the loss function, note we need to transpose again\n",
    "        # since the loss function expects each observation is a row\n",
    "        # TODO: Fix the shapes of everything so consistent, this is ugly and not readable\n",
    "        l, g = self.loss(y.T, layer_outputs[-1].T, grad=True)\n",
    "        layer_outputs.append(l)\n",
    "        grads.append(g)\n",
    "        \n",
    "        return layer_outputs, grads\n",
    "        \n",
    "        \n",
    "    def _backward(self, layer_outputs, grads):\n",
    "        \"\"\"\n",
    "        Backward pass through the network, computing gradients at each layer and storing the update factor\n",
    "        \"\"\"    \n",
    "        \n",
    "        # initialise objects to store gradients for weights and biases\n",
    "        weights_delta = [np.empty_like(w) for w in self.weights]\n",
    "        biases_delta = [np.empty_like(b) for b in self.biases]\n",
    "        \n",
    "        # handle loss layer since special case\n",
    "        loss, layer_outputs = layer_outputs[-1], layer_outputs[:-1]   # remove value of loss function from layer_outputs\n",
    "        loss_grad, grads = grads[-1], grads[:-1]                      # remove gradient of loss from grads \n",
    "        \n",
    "        # TODO: this is ugly and might break, must be a better way\n",
    "        delta = loss_grad * grads[-1] if grads[-1].shape[1] == 1 \\\n",
    "                                      else grads[-1] @ loss_grad\n",
    "        biases_delta[-1] = delta\n",
    "        weights_delta[-1] = delta @ layer_outputs[-2].T\n",
    "        \n",
    "        # now step through remaining layers\n",
    "        for i in range(2, len(layer_outputs)):\n",
    "            delta = self.weights[-i+1].T @ delta * grads[-i]\n",
    "            biases_delta[-i] = delta\n",
    "            weights_delta[-i] = delta @ layer_outputs[-i-1].T \n",
    "            \n",
    "            \n",
    "        return weights_delta, biases_delta\n",
    "        \n",
    "    def _two_pass(self, x, y):\n",
    "        \"\"\"\n",
    "        Forward pass then backward pass and return update deltas for all parameters\n",
    "        \"\"\"\n",
    "        layer_outputs, grads = self._forward(x, y)\n",
    "        weights_delta, biases_delta = self._backward(layer_outputs, grads)\n",
    "        return weights_delta, biases_delta\n",
    "    \n",
    "    @staticmethod\n",
    "    def _logistic(x: np.ndarray, grad: bool=False):\n",
    "        \"\"\"\n",
    "        Compute logistic function for input x. Optionally compute gradient at x\n",
    "        \"\"\"\n",
    "        ex = np.exp(-x)\n",
    "        f = 1 / (1 + ex)\n",
    "        if grad:\n",
    "            g = ex / (1 + ex)**2\n",
    "            return f, g\n",
    "        else: \n",
    "            return f\n",
    "    \n",
    "    @staticmethod\n",
    "    def _tanh(x: np.ndarray, grad: bool=False):\n",
    "        \"\"\"\n",
    "        Compute hyperbolic tangent function for input x. Optionally compute gradient at x\n",
    "        \"\"\"\n",
    "        ex = np.exp(2 * x)\n",
    "        f = (ex - 1) / (ex + 1)\n",
    "        if grad:\n",
    "            g = 1 - f ** 2\n",
    "            return f, g\n",
    "        else:\n",
    "            return f\n",
    "\n",
    "    @staticmethod\n",
    "    def _relu(x: np.ndarray, grad: bool=False):\n",
    "        \"\"\"\n",
    "        Compute relu function at x. Optionally compute gradient at x\n",
    "        \"\"\"\n",
    "        f = np.maximum(0, x)\n",
    "        if grad:\n",
    "            g = np.sign(x)\n",
    "            return f, g\n",
    "        else:\n",
    "            return f\n",
    "    \n",
    "    @staticmethod\n",
    "    def _linear(x, grad):\n",
    "        \"\"\"\n",
    "        Compute linear function at x\n",
    "        \"\"\"\n",
    "        if grad:\n",
    "            return x, np.ones_like(x)\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def _softmax(x: np.ndarray, grad: bool=False):\n",
    "        \"\"\"\n",
    "        Compute softmax for input x. Optionally compute grad (more accurately Jacobian). Note if input is N x 1, Jacobian is N x N matrix\n",
    "        \"\"\"\n",
    "        ex = np.exp(x)\n",
    "        f = ex / np.sum(ex, axis=0)\n",
    "        if grad:\n",
    "            g =  np.diagflat(f) - np.outer(f,f)\n",
    "            return f, g\n",
    "        else:\n",
    "            return f\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def _categorical_cross_entropy(y: np.ndarray, y_hat: np.ndarray, grad=False, epsilon=1e-4):\n",
    "        \"\"\"\n",
    "        Compute the cross entropy loss for set of predictions y_hat and labels y. Assumes y is one hot encoded, and y_hat contains class probs.\n",
    "        Assumes y and y_hat have shape (n_observation, n_class)\n",
    "        \"\"\"\n",
    "        y_hat_clipped = np.clip(y_hat, 0+epsilon, 1)\n",
    "        l = np.mean(-np.log(y_hat_clipped[y==1]))\n",
    "        if grad:\n",
    "            g = np.mean(-1 * (1 / y_hat_clipped) * y, axis=0).reshape(-1,1)\n",
    "            return l, g\n",
    "        else:\n",
    "            return l\n",
    "        \n",
    "    @staticmethod\n",
    "    def _cross_entropy(y, y_hat, grad=False, epsilon=1e-4):\n",
    "        \"\"\"\n",
    "        Compute cross entropy loss for binary response task\n",
    "        \"\"\"\n",
    "        y_hat_clipped = np.clip(y_hat, 0+epsilon, 1-epsilon)\n",
    "        l = - np.sum(y * np.log(y_hat_clipped) + (1 - y) * np.log(1 - y_hat_clipped))\n",
    "        if grad:\n",
    "            g = - (y * (1 / y_hat_clipped) - (1 - y) * (1 / (1 - y_hat_clipped))).reshape(-1,1)\n",
    "            return l, g\n",
    "        else:\n",
    "            return l\n",
    "    \n",
    "    @staticmethod\n",
    "    def _mean_square_error(y, y_hat, grad=False):\n",
    "        \"\"\"\n",
    "        Compute mean square error loss, and optionally return gradient at y_hat\n",
    "        \"\"\"\n",
    "        l = np.mean((y - y_hat) ** 2)\n",
    "        if grad:\n",
    "            g = (2 * (y_hat - y))\n",
    "            return l, g\n",
    "        else:\n",
    "            return l\n",
    "        \n",
    "    @staticmethod\n",
    "    def _mean_absolute_error(y, y_hat, grad=False):\n",
    "        \"\"\"\n",
    "        Compute mean absolute error loss, and optionally return gradient at y_hat\n",
    "        \"\"\"\n",
    "        l = np.mean(np.abs(y - y_hat))\n",
    "        if grad:\n",
    "            g = np.sign(y_hat - y).T\n",
    "            return l, g\n",
    "        else:\n",
    "            return l\n",
    "    \n",
    "    @staticmethod\n",
    "    def _accuracy(y, y_hat):\n",
    "        \"\"\"\n",
    "        compute accuracy for classification task\n",
    "        \"\"\"\n",
    "        if y_hat.ndim==1:\n",
    "            a = np.mean(1.0 * (y == y_hat))\n",
    "        elif y_hat.ndim==2:\n",
    "            a = np.mean(1.0 * (np.argmax(y, axis=1) == np.argmax(y_hat, axis=1)))\n",
    "        else:\n",
    "            raise ValueError(f\"y_hat and y are not appropriate shape. y_hat has shape: {y_hat.shape}, y has shape: {y.shape}\")\n",
    "        return a\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Example\n",
    "---\n",
    "XOR is a simple boolean function which is not linearly separable, lets see if this MLP can solve it by using a hidden layer. Then solve the same problem using keras API with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set up the data, only four possible combinations\n",
    "# note we represent the binary outcome as a 2 element row \n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[1,0],[0,1],[0,1],[1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# initialise the MLP and train it, then plot the loss and look at predictions\n",
    "\n",
    "m = MultiLayerPerceptron(layers=[2,2,2], activations=['tanh','softmax'], loss='categorical_cross_entropy')\n",
    "history = m.stochastic_gradient_descent(X, Y, batch_size=4, epochs=10000, eta=0.1, n_print=2000)\n",
    "plt.plot(history['loss'])\n",
    "print(m.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# we can try on the binary version of outputs to make sure that also works\n",
    "# it seems like it struggles a lot more to solve with binary output like this\n",
    "\n",
    "Y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "m2 = MultiLayerPerceptron(layers=[2,2,1], activations=['tanh','logistic'], loss='mean_absolute_error')\n",
    "history2 = m2.stochastic_gradient_descent(X, Y, batch_size=4, epochs=10000, eta=0.01, n_print=2000)\n",
    "plt.plot(history2['loss'])\n",
    "print(m2.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# can try the same thing with an extra layer - maybe the single output loses some flexibility\n",
    "\n",
    "m3 = MultiLayerPerceptron(layers=[2,3,3,1], activations=['tanh','tanh','logistic'], loss='cross_entropy')\n",
    "history3 = m3.stochastic_gradient_descent(X, Y, batch_size=4, epochs=10000, eta=0.1, n_print=2000)\n",
    "plt.plot(history3['loss'])\n",
    "print(m3.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can do the same thing using TensorFlow API\n",
    "\n",
    "Solve the same simple problem using TF api, see if behaviour is significantly different or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# first try with the original multicategorical output\n",
    "\n",
    "Y = np.array([[1,0],[0,1],[0,1],[1,0]])\n",
    "\n",
    "m = keras.models.Sequential()\n",
    "m.add(keras.layers.InputLayer(input_shape=X[0].shape))\n",
    "m.add(keras.layers.Dense(2, activation=keras.activations.tanh))\n",
    "m.add(keras.layers.Dense(2, activation=keras.activations.softmax))\n",
    "m.compile(loss=keras.losses.CategoricalCrossentropy(), optimizer=keras.optimizers.SGD(learning_rate=0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "history = m.fit(X, Y, epochs=10000, batch_size=4, verbose=0)\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# seemed to solve problem well\n",
    "m.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# what if we try on univarite binary outcomes\n",
    "\n",
    "Y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "m = keras.models.Sequential()\n",
    "m.add(keras.layers.InputLayer(input_shape=X[0].shape))\n",
    "m.add(keras.layers.Dense(2, activation=keras.activations.tanh))\n",
    "m.add(keras.layers.Dense(1, activation=keras.activations.sigmoid))\n",
    "m.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=keras.optimizers.SGD(learning_rate=0.1))\n",
    "\n",
    "history = m.fit(X,Y,epochs=10000,batch_size=4,verbose=0)\n",
    "plt.plot(history.history['loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# seems to have same issue as above - dooesn't solve correctly\n",
    "m.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# try adding extra layer just as we did with custom implementation\n",
    "\n",
    "m = keras.models.Sequential()\n",
    "m.add(keras.layers.InputLayer(input_shape=X[0].shape))\n",
    "m.add(keras.layers.Dense(3, activation=keras.activations.tanh))\n",
    "m.add(keras.layers.Dense(3, activation=keras.activations.tanh))\n",
    "m.add(keras.layers.Dense(1, activation=keras.activations.sigmoid))\n",
    "m.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=keras.optimizers.SGD(learning_rate=0.1))\n",
    "\n",
    "history = m.fit(X, Y, epochs=10000, batch_size=4, verbose=0)\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# works very well\n",
    "m.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try on more interesting data set\n",
    "---\n",
    "\n",
    "Load the fashion MNIST data set which is 60,000 28 x 28 greyscale pictures of clothing items, and associated class\n",
    "\n",
    "We will try to solve the classification proble using both custom implementation and TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# get data using keras utility\n",
    "# this contains 60,000 28 x 28 ndarrays with values between 0 and 255, representing pixel intensities\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# split train full into train and valid, and normalise so input is float between 0 and 1\n",
    "x_valid, x_train = x_train_full[:5000] / 255.0, x_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# manually type out the class names \n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# first we will solve using keras\n",
    "\n",
    "m = keras.models.Sequential()\n",
    "m.add(keras.layers.Flatten(input_shape=x_train[0].shape))\n",
    "m.add(keras.layers.Dense(300, activation=keras.activations.relu))\n",
    "m.add(keras.layers.Dense(100, activation=keras.activations.relu))\n",
    "m.add(keras.layers.Dense(len(class_names), activation=keras.activations.softmax))\n",
    "\n",
    "m.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "m.fit(x=x_train, y=y_train, epochs=1, validation_data=(x_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# now try using custom implementation\n",
    "\n",
    "y_train_onehot, y_valid_onehot = keras.utils.to_categorical(y_train), keras.utils.to_categorical(y_valid)\n",
    "x_train_flat, x_valid_flat = x_train.reshape(x_train.shape[0],-1), x_valid.reshape(x_valid.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "m2 = MultiLayerPerceptron(layers=[x_train[0].shape[0] ** 2, 300, 200, 100, len(class_names)],\n",
    "                          activations=['tanh','tanh','tanh','softmax'],\n",
    "                          loss='categorical_cross_entropy')\n",
    "\n",
    "history2 = m2.stochastic_gradient_descent(x=x_train_flat, y=y_train_onehot,\n",
    "                                          batch_size=128, epochs=10,\n",
    "                                          n_print=1,\n",
    "                                          valid_data=(x_valid_flat,y_valid_onehot),\n",
    "                                         metric='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2['metric']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on adding support for mini batch backprop (WIP)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# delta = loss_grad * grads[-1] if grads[-1].shape[1] == 1 \\\n",
    "#                               else grads[-1] @ loss_grad\n",
    "# biases_delta[-1] = delta\n",
    "# weights_delta[-1] = delta @ layer_outputs[-2].T\n",
    "\n",
    "def test_softmax(x, grad=False):\n",
    "    \"\"\"\n",
    "    Compute softmax for input x. Optionally compute grad (more accurately Jacobian). Note if input is N x 1, Jacobian is N x N matrix\n",
    "    \"\"\"\n",
    "    ex = np.exp(x)\n",
    "    f = ex / np.sum(ex, axis=0)\n",
    "    if grad:\n",
    "        n_obs = x.shape[1]      # each column is an instance\n",
    "        outers = ex.reshape(n_obs, -1, 1) @ ex.reshape(n_obs, 1, -1)\n",
    "        diags = np.apply_along_axis(np.diag, -1, ex.T)\n",
    "        g =  diags - outers\n",
    "        return f, g\n",
    "    else:\n",
    "        return f\n",
    "    \n",
    "\n",
    "# what happends when we apply a function (eg softmax deriv): R^n x 1 -> R^n x R^n (jacobian) to a \n",
    "# n x m matrix (eg each observation is one column)\n",
    "\n",
    "# just make a MLP to get access to softmax method\n",
    "m = MultiLayerPerceptron(layers=[2,2,2], activations=['tanh','softmax'], loss='categorical_cross_entropy')\n",
    "\n",
    "instance = np.array([[0,0], [1,0],[0,1],[1,1,]]).T\n",
    "# instance\n",
    "_, loss_grad = m._mean_square_error(instance.T, instance.T, True)\n",
    "\n",
    "_, softmax_grad = test_softmax(instance, True)\n",
    "\n",
    "loss_grad[:,:,None].shape, softmax_grad.shape\n",
    "\n",
    "(softmax_grad[:,:,:] @ loss_grad[:,:,None]).shape\n",
    "\n",
    "instance_reshaped_columns = instance.reshape(instance.shape[1],-1,1)\n",
    "instance_reshaped_rows = instance.T.reshape(instance.shape[1],1,-1)\n",
    "\n",
    "instance_reshaped_columns.shape, instance_reshaped_rows.shape\n",
    "\n",
    "(instance_reshaped_columns @ instance_reshaped_rows).shape\n",
    "\n",
    "instance\n",
    "np.apply_along_axis(np.diag, -1, instance.T)\n",
    "# np.apply()\n",
    "\n",
    "x = np.array([[1,2,],[5,6], [7,8]])\n",
    "np.apply_along_axis(lambda x: np.diag(x), 1, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
