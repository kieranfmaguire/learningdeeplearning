{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "generate_text_with_RNN_TF.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kieranfmaguire/learningdeeplearning/blob/main/notebooks/generate_text_with_RNN_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9ABIixKMVUG"
      },
      "source": [
        "# Generating Shakespearean Text Using a Character RNN\n",
        "---\n",
        "Based on example in chapter 16 - Hands on Machine Learnings with Scikitlearn and Tensorflow (O'Reilly)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL_vOkc1MVUI"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvmcQrl0MVUL"
      },
      "source": [
        "## Downloading some data\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls11I8KLMVUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f99f5a-840c-480b-f6d4-e79aceb7e85f"
      },
      "source": [
        "shakespeare_url = \"https://homl.info/shakespeare\"\n",
        "filepath = tf.keras.utils.get_file(\"shakespear.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()\n",
        "\n",
        "print(f\"shakespeare_text is a: {type(shakespeare_text)}\")\n",
        "print(f\"shakespeare_text has length: {len(shakespeare_text)}\")\n",
        "print(f\"First 500 characters are:\\n\\n{shakespeare_text[:500]}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shakespeare_text is a: <class 'str'>\n",
            "shakespeare_text has length: 1115394\n",
            "First 500 characters are:\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOp_WFo4MVUX"
      },
      "source": [
        "## Tokenizing the text data - each unique character is given a unique ID\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf3KuIZvMVUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c00549-40dc-449e-a71a-b4b7c99e328d"
      },
      "source": [
        "tokeniser = tf.keras.preprocessing.text.Tokenizer(char_level=True)   # char level creates unique id for each charater (default is by word)\n",
        "tokeniser.fit_on_texts(shakespeare_text)  # assigns ID (starting at 1) for each unique character\n",
        "num_unique_chars = len(tokeniser.word_index)\n",
        "num_chars = tokeniser.document_count    # will need this later for one hot encoding\n",
        "\n",
        "print(f\"Number of distinct characters: {len(tokeniser.word_index)}\")\n",
        "print(f\"Total number of characters in documents: {num_chars}\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of distinct characters: 39\n",
            "Total number of characters in documents: 1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLXG1rqbMVUX"
      },
      "source": [
        "# now encode the entire text, split into train and test, and create tf.data.Dataset object\n",
        "encoded = np.array(tokeniser.texts_to_sequences([shakespeare_text])).flatten()\n",
        "\n",
        "train_size = int(encoded.shape[0] * 0.9)    # use 90% for train, save the rest for test\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices(encoded[train_size:])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNUv0HFkMVUY"
      },
      "source": [
        "### To train the model, will use sub-sequences of length `n_steps`\n",
        "---\n",
        "the tensorflow datasets API can handle this with the window method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUdkRgLxMVUY"
      },
      "source": [
        "n_steps = 50                    # number of characters in a given instance\n",
        "window_length = n_steps + 1     # we will predict the next character in the sequence, so will need it as the target\n",
        "batch_size = 32                 # batch size for gradient descent"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ3Gps2oMVUY"
      },
      "source": [
        "dataset = dataset.window(size=window_length, shift=1, stride=1, drop_remainder=False)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "dataset_val = dataset_val.window(size=window_length, shift=1, stride=1, drop_remainder=False)\n",
        "dataset_val = dataset_val.flat_map(lambda window: window.batch(window_length))\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsjU_5S-MVUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c8df47-b6a6-4e47-acd3-8c978b8c4779"
      },
      "source": [
        "# have a look at the structure\n",
        "for i, dat in enumerate(dataset):\n",
        "    print(dat)\n",
        "    if i > 0: break"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[20  6  9  8  3  1 19  6  3  6 36  2 10 24 11 22  2 20  4  9  2  1 17  2\n",
            "  1 23  9  4 19  2  2 13  1  5 10 16  1 20 14  9  3  7  2  9 18  1  7  2\n",
            "  5  9  1], shape=(51,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[ 6  9  8  3  1 19  6  3  6 36  2 10 24 11 22  2 20  4  9  2  1 17  2  1\n",
            " 23  9  4 19  2  2 13  1  5 10 16  1 20 14  9  3  7  2  9 18  1  7  2  5\n",
            "  9  1 15], shape=(51,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X5oC2XgMVUZ"
      },
      "source": [
        "# shuffle, batch, then split into fetaures and target\n",
        "dataset = dataset.shuffle(int(1e3)).batch(batch_size)\n",
        "dataset = dataset.map(lambda x: (x[:, :-1], x[:, 1:]))   \n",
        "# one hot encode the categorical features - small enough dictionary for this to be reasonable. \n",
        "# in general this would usually be an embedding\n",
        "dataset = dataset.map(lambda x, y: (tf.one_hot(x, depth=num_unique_chars), y))\n",
        "dataset = dataset.prefetch(1) # don't really need to worry about prefetching on this small dataset\n",
        "\n",
        "dataset_val = dataset_val.batch(batch_size)\\\n",
        "  .map(lambda x: (x[:, :-1], x[:, 1:]))\\\n",
        "  .map(lambda x,y: (tf.one_hot(x, depth=num_unique_chars), y))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiqDzM1uMVUZ"
      },
      "source": [
        "## Now we can create a model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS0br1wyMVUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c270b7e-c153-4969-d64d-69239e349645"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.GRU(units=64, input_shape=[None, num_unique_chars], return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
        "  tf.keras.layers.GRU(units=64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
        "  tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=num_unique_chars, activation=\"softmax\"))\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_8 (GRU)                  (None, None, 64)          20160     \n",
            "_________________________________________________________________\n",
            "gru_9 (GRU)                  (None, None, 64)          24960     \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, None, 39)          2535      \n",
            "=================================================================\n",
            "Total params: 47,655\n",
            "Trainable params: 47,655\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqsPqUieduTg",
        "outputId": "be68b3ed-0344-4eda-fb2f-aa933a8615a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "model.fit(dataset, epochs=1, validation_data=dataset_val)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  21424/Unknown - 2413s 113ms/step - loss: 1.7703"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-65d6be91bd76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  Received a label value of 39 which is outside the valid range of [0, 39).  Label values: 1 7 5 3 7 1 5 12 12 5 16 28 13 1 3 7 2 6 9 1 8 17 2 12 12 6 10 21 1 21 9 6 2 20 8 18 11 15 16 1 15 2 9 19 16 1 13 9 6 2 7 1 21 9 2 5 3 1 8 14 22 8 6 13 6 2 8 27 11 10 4 9 1 20 4 9 17 5 9 13 1 4 20 1 9 2 26 2 10 21 2 18 1 3 7 4 14 21 7 1 26 6 24 11 17 2 12 12 32 15 6 10 13 2 13 1 19 12 5 9 2 10 19 2 18 1 22 2 1 3 7 4 14 1 20 4 9 3 14 10 5 3 2 31 11 11 15 4 10 3 29 1 15 16 1 15 2 2 13 1 7 5 3 7 1 21 4 3 1 15 2 1 20 5 15 2 24 11 6 1 7 5 26 2 1 10 4 3 1 8 3 4 23 23 28 13 1 15 6 10 11 7 2 9 2 1 5 3 1 3 7 2 1 23 5 12 5 19 2 1 6 1 17 6 12 12 1 9 2 8 3 1 5 17 7 6 12 2 27 11 19 4 14 8 6 10 1 4 20 1 1 17 4 10 13 9 4 14 8 1 17 2 12 12 1 22 2 12 4 26 2 13 18 11 6 10 1 4 35 20 4 9 13 8 7 6 9 2 1 8 7 5 12 3 1 15 14 8 3 2 26 2 10 21 2 18 1 3 7 4 14 21 7 1 3 7 2 16 1 15 14 19 7 1 2 9 9 28 13 24 11 3 7 2 10 1 17 7 16 1 8 7 4 14 12 13 1 3 7 2 19 2 1 5 21 5 6 10 1 23 9 4 19 12 5 6 15 1 14 8 1 25 6 10 21 1 4 20 1 2 10 21 12 5 10 13 27 11 16 4 14 1 5 9 2 1 3 7 2 1 23 9 2 8 8 28 13 1 3 7 2 15 1 17 6 3 7 1 21 9 2 5 3 1 8 14 22 8 6 13 6 2 8 27 11 10 4 9 1 20 4 9 17 5 9 13 1 4 20 1 10 2 1 2 5 9 8 1 3 4 1 3 7 2 6 9 1 13 2 15 5 10 13 8 18 11 10 4 9 1 23 4 8 3 2 13 1 4 20 20 1 3 7 2 6 9 1 8 14 6 19 4 14 8 6 10 1 4 20 1 2 35 2 3 2 9 18 1 17 7 5 3 1 3 7 6 10 25 8 1 16 4 14 9 1 12 4 9 13 8 7 6 23 30 11 15 2 3 7 6 6 10 1 4 20 1 2 35 2 3 2 9 18 1 17 7 5 3 1 3 7 6 10 25 8 1 16 4 14 9 1...\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-33-65d6be91bd76>:1) ]] [Op:__inference_train_function_13223]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    }
  ]
}