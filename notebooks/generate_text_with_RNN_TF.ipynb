{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Generating Shakespearean Text Using a Character RNN\n",
    "---\n",
    "Based on example in chapter 16 - Hands on Machine Learnings with Scikitlearn and Tensorflow (O'Reilly)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "source": [
    "## Downloading some data\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shakespeare_text is a: <class 'str'>\nshakespeare_text has length: 1115394\nFirst 500 characters are:\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor\n"
     ]
    }
   ],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = tf.keras.utils.get_file(\"shakespear.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "print(f\"shakespeare_text is a: {type(shakespeare_text)}\")\n",
    "print(f\"shakespeare_text has length: {len(shakespeare_text)}\")\n",
    "print(f\"First 500 characters are:\\n\\n{shakespeare_text[:500]}\")"
   ]
  },
  {
   "source": [
    "## Tokenizing the text data - each unique character is given a unique ID\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of distinct characters: 39\nTotal number of characters in documents: 1115394\n"
     ]
    }
   ],
   "source": [
    "tokeniser = tf.keras.preprocessing.text.Tokenizer(char_level=True)   # char level creates unique id for each charater (default is by word)\n",
    "tokeniser.fit_on_texts(shakespeare_text)  # assigns ID (starting at 1) for each unique character\n",
    "num_chars = tokeniser.document_count    # will need this later for one hot encoding\n",
    "\n",
    "print(f\"Number of distinct characters: {len(tokeniser.word_index)}\")\n",
    "print(f\"Total number of characters in documents: {num_chars}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now encode the entire text, split into train and test, and create tf.data.Dataset object\n",
    "encoded = np.array(tokeniser.texts_to_sequences([shakespeare_text])).flatten()\n",
    "\n",
    "train_size = int(encoded.shape[0] * 0.9)    # use 90% for train, save the rest for test\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "source": [
    "### To train the model, will use sub-sequences of length `n_steps`\n",
    "---\n",
    "the tensorflow datasets API can handle this with the window method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50                    # number of characters in a given instance\n",
    "window_length = n_steps + 1     # we will predict the next character in the sequence, so will need it as the target\n",
    "batch_size = 32                 # batch size for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.window(size=window_length, shift=1, stride=1, drop_remainder=False)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n[20  6  9  8  3  1 19  6  3  6 36  2 10 24 11 22  2 20  4  9  2  1 17  2\n  1 23  9  4 19  2  2 13  1  5 10 16  1 20 14  9  3  7  2  9 18  1  7  2\n  5  9  1], shape=(51,), dtype=int64)\ntf.Tensor(\n[ 6  9  8  3  1 19  6  3  6 36  2 10 24 11 22  2 20  4  9  2  1 17  2  1\n 23  9  4 19  2  2 13  1  5 10 16  1 20 14  9  3  7  2  9 18  1  7  2  5\n  9  1 15], shape=(51,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# have a look at the structure\n",
    "for i, dat in enumerate(dataset):\n",
    "    print(dat)\n",
    "    if i > 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle, batch, then split into fetaures and target\n",
    "dataset = dataset.shuffle(1e5).batch(batch_size).map(lambda x: x[:-1], x[-1])   \n",
    "# one hot encode the categorical features - small enough dictionary for this to be reasonable. \n",
    "# in general this would usually be an embedding\n",
    "dataset = dataset.map(lammbda x, y: tf.one_hot(x, depth=num_chars), y)\n",
    "dataset = dataset.prefetch(1) # don't really need to worry about prefetching on this small dataset"
   ]
  },
  {
   "source": [
    "## Now we can create a model\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}